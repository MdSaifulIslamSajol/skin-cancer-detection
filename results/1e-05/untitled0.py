#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Apr  9 16:20:03 2024

@author: saiful
"""

# -*- coding: utf-8 -*-
"""Skin lesion classification(acc>90%)(Pytorch 5bb988

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/skin-lesion-classification-acc-90-pytorch-5bb988-5221bbb5-a95a-4110-bbe5-04cefea73cde.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240409/auto/storage/goog4_request%26X-Goog-Date%3D20240409T211906Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3De3cadd33649e66dd78319e28c9e41214019972dc81e76168d028ae729432af65179012dba040e4701e0f02a1e8405fc50887ac3f4dcb20807d5fa7bbbf3a2f52cc650726f7fa55d45d398290f9c30d5cf8fa321c3bb0bc635a90f9657494fc57243e10421552df79df9e1e691055bb7ff9c797455a68fe4d58cad9329e26ee4609f2b9b7e03e31587ad4a0ee3124c4a6b4d80cc58b61951621f09d36017263ade59011f1f9281e149620e5247a4d8bd7174e69a0fa42c10f1b5f0b0aca3be7be2b788adf8513ac9b20b44c06058a98273369c1c4ccc1838c12533f450eef332b2d35ee54f6ba0aa095345e4c455ad7552570e2cecfab58837f1584a63502f27b
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = ':https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F54339%2F104884%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240409%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240409T211906Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D55292f812dfa08c1f4e6533e3faeffdb9eeff7321c1669475a3b2eb08f92fc82021541d9c60f54da26f3b8fd45fffc3f28596ff807727c585cdb27737162b66c5a62f5d3ace450538c5a2baacdc4353b1787f86451d8842cde776c74260456daf334de43cd492734be0d1481f12947ebcaf73ddcb2863e13fe34b9b63fcb483c6abc8b4c04deb90fb8c0aa587fd42d07788e73dbf663f69928c703554c98c162184279f6d3f1cb2957c12db6773d1bdb9d3dc8f5e4a92afa31f81f2423373519c173c9a54d1077decabb2c7c95e484707f419ee8f4aa1005954dc54ddd3282950174123e458e5701942e6055bef4f8b8d53f2e370d426e52dfd71972a5d3d495'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""This is a very interesting task and it's very suitable for beginners, through this task you can learn about data analysis, data processing, model building, model training, parameter optimization and so on. When only use the images, with a common network, such as Resnet, Densenet, you can achieve a relatively good accuracy very easily.

By analyzing the data, the basic information of the patient is also related to the classification of the diseased tissue. Therefore, if we can combine the case information to carry out the classification task, it will be a very meaningful work. Actually during clinical diagnosis, doctors will also combine different modal data to make comprehensive judgments.

Due to the urgency of time, my current method only uses image data, and then I will consider adding the patient's personal information to the classification task to train a more complete model. I will update my kernel immediately once I finished.

Before you really start, I strongly recommend you to read the material of pigmented lesions and dermatoscopic images[https://arxiv.org/abs/1803.10417]. After that, you can learn about the characteristics and distribution of the data from the task description and this kernel[https://www.kaggle.com/kmader/dermatology-mnist-loading-and-processing]

In this kernel I have followed following steps for model building and evaluation:

> Step 1. Data analysis and preprocessing

> Step 2. Model building

> Step 3. Model training

> Step 4. Model evaluation

I used the pytorch framework to complete the entire task. The code contains several common networks, such as Resnet, VGG, Densenet, and Inception. You only need to make minor changes on the code to complete the network switch. Without the hyperparameter adjustment, I used **Densenet-121 to achieve an accuracy of more than 90% on the validation set in 10 epochs.**

### First, import all libraries that used in this project
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# python libraties
import os, cv2,itertools
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm import tqdm
from glob import glob
from PIL import Image

# pytorch libraries
import torch
from torch import optim,nn
from torch.autograd import Variable
from torch.utils.data import DataLoader,Dataset
from torchvision import models,transforms

# sklearn libraries
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# to make the results are reproducible
np.random.seed(10)
torch.manual_seed(10)
torch.cuda.manual_seed(10)

print(os.listdir("../input"))

"""## Step 1. Data analysis and preprocessing

Get the all image data pathsï¼Œ match the row information in HAM10000_metadata.csv with its corresponding image
"""

data_dir = '../input'
all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))
imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}
lesion_type_dict = {
    'nv': 'Melanocytic nevi',
    'mel': 'dermatofibroma',
    'bkl': 'Benign keratosis-like lesions ',
    'bcc': 'Basal cell carcinoma',
    'akiec': 'Actinic keratoses',
    'vasc': 'Vascular lesions',
    'df': 'Dermatofibroma'
}

"""This function is used to compute the mean and standard deviation on the whole dataset, will use for inputs normalization"""

def compute_img_mean_std(image_paths):
    """
        computing the mean and std of three channel on the whole dataset,
        first we should normalize the image from 0-255 to 0-1
    """

    img_h, img_w = 224, 224
    imgs = []
    means, stdevs = [], []

    for i in tqdm(range(len(image_paths))):
        img = cv2.imread(image_paths[i])
        img = cv2.resize(img, (img_h, img_w))
        imgs.append(img)

    imgs = np.stack(imgs, axis=3)
    print(imgs.shape)

    imgs = imgs.astype(np.float32) / 255.

    for i in range(3):
        pixels = imgs[:, :, i, :].ravel()  # resize to one row
        means.append(np.mean(pixels))
        stdevs.append(np.std(pixels))

    means.reverse()  # BGR --> RGB
    stdevs.reverse()

    print("normMean = {}".format(means))
    print("normStd = {}".format(stdevs))
    return means,stdevs

"""Return the mean and std of RGB channels"""

norm_mean,norm_std = compute_img_mean_std(all_image_path)

"""Add three columns to the original DataFrame, path (image path), cell_type (the whole name),cell_type_idx (the corresponding index  of cell type, as the image label )"""

df_original = pd.read_csv(os.path.join(data_dir, 'HAM10000_metadata.csv'))
df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)
df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)
df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes
df_original.head()

# this will tell us how many images are associated with each lesion_id
df_undup = df_original.groupby('lesion_id').count()
# now we filter out lesion_id's that have only one image associated with it
df_undup = df_undup[df_undup['image_id'] == 1]
df_undup.reset_index(inplace=True)
df_undup.head()

# here we identify lesion_id's that have duplicate images and those that have only one image.
def get_duplicates(x):
    unique_list = list(df_undup['lesion_id'])
    if x in unique_list:
        return 'unduplicated'
    else:
        return 'duplicated'

# create a new colum that is a copy of the lesion_id column
df_original['duplicates'] = df_original['lesion_id']
# apply the function to this new column
df_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)
df_original.head()

df_original['duplicates'].value_counts()

# now we filter out images that don't have duplicates
df_undup = df_original[df_original['duplicates'] == 'unduplicated']
df_undup.shape

# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set
y = df_undup['cell_type_idx']
_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)
df_val.shape

df_val['cell_type_idx'].value_counts()

# This set will be df_original excluding all rows that are in the val set
# This function identifies if an image is part of the train or val set.
def get_val_rows(x):
    # create a list of all the lesion_id's in the val set
    val_list = list(df_val['image_id'])
    if str(x) in val_list:
        return 'val'
    else:
        return 'train'

# identify train and val rows
# create a new colum that is a copy of the image_id column
df_original['train_or_val'] = df_original['image_id']
# apply the function to this new column
df_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)
# filter out train rows
df_train = df_original[df_original['train_or_val'] == 'train']
print(len(df_train))
print(len(df_val))

df_train['cell_type_idx'].value_counts()

df_val['cell_type'].value_counts()

"""**From From the above statistics of each category, we can see that there is a serious class imbalance in the training data. To solve this problem, I think we can start from two aspects, one is equalization sampling, and the other is a loss function that can be used to mitigate category imbalance during training, such as focal loss.**"""

# Copy fewer class to balance the number of 7 classes
data_aug_rate = [15,10,5,50,0,40,5]
for i in range(7):
    if data_aug_rate[i]:
        df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)
df_train['cell_type'].value_counts()

"""At the beginning, I divided the data into three parts, training set, validation set and test set. Considering the small amount of data, I did not further divide the validation set data in practice."""

# # We can split the test set again in a validation set and a true test set:
# df_val, df_test = train_test_split(df_val, test_size=0.5)
df_train = df_train.reset_index()
df_val = df_val.reset_index()
# df_test = df_test.reset_index()

"""## Step 2. Model building"""

# feature_extract is a boolean that defines if we are finetuning or feature extracting.
# If feature_extract = False, the model is finetuned and all model parameters are updated.
# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.
def set_parameter_requires_grad(model, feature_extracting):
    if feature_extracting:
        for param in model.parameters():
            param.requires_grad = False

def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):
    # Initialize these variables which will be set in this if statement. Each of these
    #   variables is model specific.
    model_ft = None
    input_size = 0

    if model_name == "resnet":
        """ Resnet18, resnet34, resnet50, resnet101
        """
        model_ft = models.resnet50(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.fc.in_features
        model_ft.fc = nn.Linear(num_ftrs, num_classes)
        input_size = 224


    elif model_name == "vgg":
        """ VGG11_bn
        """
        model_ft = models.vgg11_bn(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.classifier[6].in_features
        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)
        input_size = 224


    elif model_name == "densenet":
        """ Densenet121
        """
        model_ft = models.densenet121(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.classifier.in_features
        model_ft.classifier = nn.Linear(num_ftrs, num_classes)
        input_size = 224

    elif model_name == "inception":
        """ Inception v3
        Be careful, expects (299,299) sized images and has auxiliary output
        """
        model_ft = models.inception_v3(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        # Handle the auxilary net
        num_ftrs = model_ft.AuxLogits.fc.in_features
        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)
        # Handle the primary net
        num_ftrs = model_ft.fc.in_features
        model_ft.fc = nn.Linear(num_ftrs,num_classes)
        input_size = 299

    elif model_name == "densenet161":
        model_ft = models.densenet161(pretrained=True)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.classifier.in_features
        model_ft.classifier = nn.Linear(num_ftrs, num_classes)
        input_size = 224


    else:
        print("Invalid model name, exiting...")
        exit()
    return model_ft, input_size

"""You can change your backbone network, here are 4 different networks, each network also has sevaral versions. Considering the limited training data, we used the ImageNet pre-training model for fine-tuning. This can speed up the convergence of the model and improve the accuracy.

There is one thing you need to pay attention to, the input size of Inception is different from the others (299x299), you need to change the setting of compute_img_mean_std() function
"""

# resnet,vgg,densenet,inception
model_name_1 = 'densenet'
num_classes = 7
feature_extract = False
# Initialize the model for this run
model_ft_1, input_size = initialize_model(model_name_1, num_classes, feature_extract, use_pretrained=True)
# Define the device:
device = torch.device('cuda:0')
# Put the model on the device:
model_1 = model_ft_1.to(device)

norm_mean = (0.49139968, 0.48215827, 0.44653124)
norm_std = (0.24703233, 0.24348505, 0.26158768)
# define the transformation of the train images.
train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),
                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),
                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),
                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])
# define the transformation of the val images.
val_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),
                                    transforms.Normalize(norm_mean, norm_std)])

# Define a pytorch dataloader for this dataset
class HAM10000(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        # Load data and get label
        X = Image.open(self.df['path'][index])
        y = torch.tensor(int(self.df['cell_type_idx'][index]))

        if self.transform:
            X = self.transform(X)

        return X, y

# Define the training set using the table train_df and using our defined transitions (train_transform)
training_set = HAM10000(df_train, transform=train_transform)
train_loader = DataLoader(training_set, batch_size=32, shuffle=True, num_workers=4)
# Same for the validation set:
validation_set = HAM10000(df_val, transform=train_transform)
val_loader = DataLoader(validation_set, batch_size=32, shuffle=False, num_workers=4)

# we use Adam optimizer, use cross entropy loss as our loss function
optimizer_1 = optim.Adam(model_1.parameters(), lr=1e-3)
criterion_1 = nn.CrossEntropyLoss().to(device)

"""## Step 3. Model training"""

# this function is used during training process, to calculation the loss and accuracy
class AverageMeter(object):
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

total_loss_train, total_acc_train = [],[]
def train(train_loader, model, criterion, optimizer, epoch):
    model.train()
    train_loss = AverageMeter()
    train_acc = AverageMeter()
    curr_iter = (epoch - 1) * len(train_loader)
    for i, data in enumerate(train_loader):
        images, labels = data
        N = images.size(0)
        # print('image shape:',images.size(0), 'label shape',labels.size(0))
        images = Variable(images).to(device)
        labels = Variable(labels).to(device)

        optimizer.zero_grad()
        outputs = model(images)

        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        prediction = outputs.max(1, keepdim=True)[1]
        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)
        train_loss.update(loss.item())
        curr_iter += 1
        if (i + 1) % 100 == 0:
            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (
                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))
            total_loss_train.append(train_loss.avg)
            total_acc_train.append(train_acc.avg)
    return train_loss.avg, train_acc.avg

def validate(val_loader, model, criterion, optimizer, epoch):
    model.eval()
    val_loss = AverageMeter()
    val_acc = AverageMeter()
    with torch.no_grad():
        for i, data in enumerate(val_loader):
            images, labels = data
            N = images.size(0)
            images = Variable(images).to(device)
            labels = Variable(labels).to(device)

            outputs = model(images)
            prediction = outputs.max(1, keepdim=True)[1]

            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)

            val_loss.update(criterion(outputs, labels).item())

    print('------------------------------------------------------------')
    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))
    print('------------------------------------------------------------')
    return val_loss.avg, val_acc.avg

epoch_num = 5
best_val_acc = 0
total_loss_val, total_acc_val = [],[]
for epoch in range(1, epoch_num+1):
    loss_train, acc_train = train(train_loader, model_1, criterion_1, optimizer_1, epoch)
    loss_val, acc_val = validate(val_loader, model_1, criterion_1, optimizer_1, epoch)
    total_loss_val.append(loss_val)
    total_acc_val.append(acc_val)
    if acc_val > best_val_acc:
        best_val_acc = acc_val
        print('*****************************************************')
        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))
        print('*****************************************************')

"""## Step 2_1. Model building"""

model_name_2 = 'resnet'
num_classes = 7
feature_extract = False
# Initialize the model for this run
model_ft_2, input_size = initialize_model(model_name_2, num_classes, feature_extract, use_pretrained=True)
# Define the device:
device = torch.device('cuda:0')
# Put the model on the device:
model_2 = model_ft_2.to(device)

optimizer_2 = optim.Adam(model_2.parameters(), lr=1e-3)
criterion_2 = nn.CrossEntropyLoss().to(device)

"""## Step 3_1. Model training"""

epoch_num = 5
best_val_acc = 0
total_loss_val, total_acc_val = [],[]
for epoch in range(1, epoch_num+1):
    loss_train, acc_train = train(train_loader, model_2, criterion_2, optimizer_2, epoch)
    loss_val, acc_val = validate(val_loader, model_2, criterion_2, optimizer_2, epoch)
    total_loss_val.append(loss_val)
    total_acc_val.append(acc_val)
    if acc_val > best_val_acc:
        best_val_acc = acc_val
        print('*****************************************************')
        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))
        print('*****************************************************')

"""## Step 2_2. Model building"""

model_name_3 = 'densenet161'
num_classes = 7
feature_extract = False
# Initialize the model for this run
model_ft_3, input_size = initialize_model(model_name_3, num_classes, feature_extract, use_pretrained=True)
# Define the device:
device = torch.device('cuda:0')
# Put the model on the device:
model_3 = model_ft_3.to(device)

optimizer_3 = optim.Adam(model_3.parameters(), lr=1e-3)
criterion_3 = nn.CrossEntropyLoss().to(device)

"""## Step 3_2. Model training"""

epoch_num = 5
best_val_acc = 0
total_loss_val, total_acc_val = [],[]
for epoch in range(1, epoch_num+1):
    loss_train, acc_train = train(train_loader, model_3, criterion_3, optimizer_3, epoch)
    loss_val, acc_val = validate(val_loader, model_3, criterion_3, optimizer_3, epoch)
    total_loss_val.append(loss_val)
    total_acc_val.append(acc_val)
    if acc_val > best_val_acc:
        best_val_acc = acc_val
        print('*****************************************************')
        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))
        print('*****************************************************')

"""## Step 4. Model evaluation"""

import torch.nn.functional as F

def single_model_acc(val_loader, model):
    model.eval()
    with torch.no_grad():
        correctly_identified = 0
        total_images = 0
        for i, data in enumerate(val_loader):
            images, labels = data
            N = images.size(0)
            images = Variable(images).to(device)
            labels = Variable(labels).to(device)

            outputs = model(images)
            for i in range(N):
                soft_max_output = F.softmax(outputs[i])
                max_index = torch.argmax(soft_max_output)
                max_value = soft_max_output[max_index]
                total_images += 1
                correctly_identified += int(labels[i] == max_index)
        print("Correctly identified = ", correctly_identified, " Total_images = ", total_images, " Accuracy = ",(float(correctly_identified)/total_images)*100)

def combined_two_models_acc(val_loader, model_1, model_2):
    model_1.eval()
    model_2.eval()
    with torch.no_grad():
        correctly_identified = 0
        total_images = 0
        for i, data in enumerate(val_loader):
            images, labels = data
            N = images.size(0)
            images = Variable(images).to(device)
            labels = Variable(labels).to(device)

            outputs_1 = model_1(images)
            outputs_2 = model_2(images)
            for i in range(N):
                soft_max_output_1 = F.softmax(outputs_1[i])
                soft_max_output_2 = F.softmax(outputs_2[i])
                max_index_1 = torch.argmax(soft_max_output_1)
                max_index_2 = torch.argmax(soft_max_output_2)
                max_value_1 = soft_max_output_1[max_index_1]
                max_value_2 = soft_max_output_2[max_index_2]
                total_images += 1

                if max_index_1 == max_index_2:
                    correctly_identified += int(labels[i] == max_index_1)
                else:
                    if max_value_1 > max_value_2:
                        correctly_identified += int(labels[i] == max_index_1)
                    else:
                        correctly_identified += int(labels[i] == max_index_2)
        print("Correctly identified = ", correctly_identified, " Total_images = ", total_images, " Accuracy = ",(float(correctly_identified)/total_images)*100)

def combined_two_models_acc_rev(val_loader, model_1, model_2):
    model_1.eval()
    model_2.eval()
    with torch.no_grad():
        correctly_identified_1 = 0
        correctly_identified_2 = 0
        correctly_identified_combined = 0
        total_images = 0
        for i, data in enumerate(val_loader):
            images, labels = data
            N = images.size(0)
            images = Variable(images).to(device)
            labels = Variable(labels).to(device)

            outputs_1 = model_1(images)
            outputs_2 = model_2(images)

            for i in range(N):
                soft_max_output_1 = F.softmax(outputs_1[i])
                soft_max_output_2 = F.softmax(outputs_2[i])
                max_index_1 = torch.argmax(soft_max_output_1)
                max_index_2 = torch.argmax(soft_max_output_2)
                max_value_1 = soft_max_output_1[max_index_1]
                max_value_2 = soft_max_output_2[max_index_2]

                correctly_identified_1 += int(labels[i] == max_index_1)
                correctly_identified_2 += int(labels[i] == max_index_2)


                total_images += 1

                if max_index_1 == max_index_2:
                    correctly_identified_combined += int(labels[i] == max_index_1)

                elif max_value_1 > max_value_2:
                    correctly_identified_combined += int(labels[i] == max_index_1)

                else:
                    correctly_identified_combined += int(labels[i] == max_index_2)


        print("Correctly identified by model 1 = ", correctly_identified_1, " Total_images = ", total_images, " Accuracy = ",(float(correctly_identified_1)/total_images)*100)
        print()
        print("Correctly identified by model 2 = ", correctly_identified_2, " Total_images = ", total_images, " Accuracy = ",(float(correctly_identified_2)/total_images)*100)
        print()
        print("Correctly identified by combined model  = ", correctly_identified_combined, " Total_images = ", total_images, " Accuracy = ",(float(correctly_identified_combined)/total_images)*100)

def combined_three_models_acc(val_loader, model_1, model_2, model_3):
    model_1.eval()
    model_2.eval()
    model_3.eval()
    with torch.no_grad():
        correctly_identified_1 = 0
        correctly_identified_2 = 0
        correctly_identified_3 = 0
        correctly_identified_combined = 0
        total_images = 0
        for i, data in enumerate(val_loader):
            images, labels = data
            N = images.size(0)
            images = Variable(images).to(device)
            labels = Variable(labels).to(device)

            outputs_1 = model_1(images)
            outputs_2 = model_2(images)
            outputs_3 = model_3(images)
            for i in range(N):
                soft_max_output_1 = F.softmax(outputs_1[i])
                soft_max_output_2 = F.softmax(outputs_2[i])
                soft_max_output_3 = F.softmax(outputs_3[i])
                max_index_1 = torch.argmax(soft_max_output_1)
                max_index_2 = torch.argmax(soft_max_output_2)
                max_index_3 = torch.argmax(soft_max_output_3)
                max_value_1 = soft_max_output_1[max_index_1]
                max_value_2 = soft_max_output_2[max_index_2]
                max_value_3 = soft_max_output_3[max_index_3]

                correctly_identified_1 += int(labels[i] == max_index_1)
                correctly_identified_2 += int(labels[i] == max_index_2)
                correctly_identified_3 += int(labels[i] == max_index_3)


                total_images += 1

                if max_index_1 == max_index_2:
                    correctly_identified_combined += int(labels[i] == max_index_1)

                elif max_index_1 == max_index_3:
                    correctly_identified_combined += int(labels[i] == max_index_1)

                elif max_index_2 == max_index_3:
                    correctly_identified_combined += int(labels[i] == max_index_2)

                elif max_value_1 > max_value_2 and max_value_1 > max_value_3:
                    correctly_identified_combined += int(labels[i] == max_index_1)

                elif max_value_2 > max_value_1 and max_value_2 > max_value_3:
                    correctly_identified_combined += int(labels[i] == max_index_2)

                else:
                    correctly_identified_combined += int(labels[i] == max_index_3)


        print("Correctly identified by model 1 = ", correctly_identified_1, " Total_images = ", total_images, " Accuracy = ",(float(correctly_identified_1)/total_images)*100)
        print()
        print("Correctly identified by model 2 = ", correctly_identified_2, " Total_images = ", total_images, " Accuracy = ",(float(correctly_identified_2)/total_images)*100)
        print()
        print("Correctly identified by model 3 = ", correctly_identified_3, " Total_images = ", total_images, " Accuracy = ",(float(correctly_identified_3)/total_images)*100)
        print()
        print("Correctly identified by combined model  = ", correctly_identified_combined, " Total_images = ", total_images, " Accuracy = ",(float(correctly_identified_combined)/total_images)*100)

single_model_acc(val_loader, model_1)

single_model_acc(val_loader, model_2)

single_model_acc(val_loader, model_3)

combined_two_models_acc(val_loader, model_1, model_2)

combined_two_models_acc_rev(val_loader, model_1, model_2)

combined_three_models_acc(val_loader, model_1, model_2, model_3)









fig = plt.figure(num = 2)
fig1 = fig.add_subplot(2,1,1)
fig2 = fig.add_subplot(2,1,2)
fig1.plot(total_loss_train, label = 'training loss')
fig1.plot(total_acc_train, label = 'training accuracy')
fig2.plot(total_loss_val, label = 'validation loss')
fig2.plot(total_acc_val, label = 'validation accuracy')
plt.legend()
plt.show()

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

model.eval()
y_label = []
y_predict = []
with torch.no_grad():
    for i, data in enumerate(val_loader):
        images, labels = data
        N = images.size(0)
        images = Variable(images).to(device)
        outputs = model(images)
        prediction = outputs.max(1, keepdim=True)[1]
        y_label.extend(labels.cpu().numpy())
        y_predict.extend(np.squeeze(prediction.cpu().numpy().T))

# compute the confusion matrix
confusion_mtx = confusion_matrix(y_label, y_predict)
# plot the confusion matrix
plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']
plot_confusion_matrix(confusion_mtx, plot_labels)

# Generate a classification report
report = classification_report(y_label, y_predict, target_names=plot_labels)
print(report)

label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)
plt.bar(np.arange(7),label_frac_error)
plt.xlabel('True Label')
plt.ylabel('Fraction classified incorrectly')

"""## Conclusion

I tried to train with different network structures. When using Densenet-121, the average accuracy of 7 classes on the validation set can reach 92% in 10 epochs. We also calculated the confusion matrix for all classes and the F1-score for each class, which is a more comprehensive indicator that can take into account both the precision and recall of the classification model.Our model can achieve more than 90% on the F1-score indicator.

Due to limited time, we did not spend much time on model training. By increasing in training epochs, adjustmenting of model hyperparameters, and attempting at different networks may further enhance the performance of the model.

## Next plan

How to use image data and patient case data at the same time, my plan is to use CNN to extract features from images, use xgboost to convert medical records into vectors and then concat them with CNN network full-layer features. Two branch networks are trained simultaneously using a loss function. We can refer to the methods used in the advertising CTR estimation task.
"""

