#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Apr  8 02:18:43 2024

@author: saiful
"""

# -*- coding: utf-8 -*-
"""Skin lesion classification(acc>90%)(Pytorch)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/skin-lesion-classification-acc-90-pytorch-2da2cc47-9d68-49da-b0ed-04364bb3c0c8.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240408/auto/storage/goog4_request%26X-Goog-Date%3D20240408T081536Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1d59b2775c360bf384d01f06870377e454bdaafb96fd457ee958fcff0a3ac9b4cb7e9acd1d3c9b080ed2c8debc9decedfeb43584f07adc61ee0ed34164a85a68a60636c0c0bcca95fc8a85e59f91a1bd5031b4afa3710ed9e545c877687f6068290fed8089198ba98ec0083939e7bc4bdf05b6b2a315c539465e83e2d4c03e6af39fecfe20cf702131410b4d406d86998d39bee41c666dc15a05560150a32d3301bc9d9fe9ffd6fef73c9c31150ab9d139e541bfd46b4ae6b2abbbd401e26ca1aaf08cf4c7985f5fd8cf17fbec61d0837e5feb2aacb89b054412fb59f3f30acad88d4f89a44e701eec70ffd8e5449806a03ad0b4b0f2e3cfc8e67379181e37f9
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import os
os.environ["CUDA_VISIBLE_DEVICES"]="3,4,5"
import sys
sys.stdout = open("console_output.txt", "w")

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# python libraties
import os, itertools
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm import tqdm
from glob import glob
from PIL import Image

# pytorch libraries
import torch
from torch import optim,nn
from torch.autograd import Variable
from torch.utils.data import DataLoader,Dataset
from torchvision import models,transforms
from my_models import initialize_model
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score


# sklearn libraries
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# to make the results are reproducible
np.random.seed(10)
torch.manual_seed(10)
torch.cuda.manual_seed(10)

# print(os.listdir("../input"))

"""## Step 1. Data analysis and preprocessing

Get the all image data pathsï¼Œ match the row information in HAM10000_metadata.csv with its corresponding image
"""

data_dir = "/data/saiful/pilot projects/HAM10000/"
all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))
imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}
lesion_type_dict = {
    'nv': 'Melanocytic nevi',
    'mel': 'dermatofibroma',
    'bkl': 'Benign keratosis-like lesions ',
    'bcc': 'Basal cell carcinoma',
    'akiec': 'Actinic keratoses',
    'vasc': 'Vascular lesions',
    'df': 'Dermatofibroma'
}

"""This function is used to compute the mean and standard deviation on the whole dataset, will use for inputs normalization"""

def compute_img_mean_std(image_paths):
    """
        computing the mean and std of three channel on the whole dataset,
        first we should normalize the image from 0-255 to 0-1
    """

    img_h, img_w = 224, 224
    imgs = []
    means, stdevs = [], []

    for i in tqdm(range(len(image_paths))):
        img = cv2.imread(image_paths[i])
        img = cv2.resize(img, (img_h, img_w))
        imgs.append(img)

    imgs = np.stack(imgs, axis=3)
    print(imgs.shape)

    imgs = imgs.astype(np.float32) / 255.

    for i in range(3):
        pixels = imgs[:, :, i, :].ravel()  # resize to one row
        means.append(np.mean(pixels))
        stdevs.append(np.std(pixels))

    means.reverse()  # BGR --> RGB
    stdevs.reverse()

    print("normMean = {}".format(means))
    print("normStd = {}".format(stdevs))
    return means,stdevs

"""Return the mean and std of RGB channels"""

#norm_mean,norm_std = compute_img_mean_std(all_image_path)
norm_mean = [0.763038, 0.54564667, 0.57004464]
norm_std = [0.14092727, 0.15261286, 0.1699712]

"""Add three columns to the original DataFrame, path (image path), cell_type (the whole name),cell_type_idx (the corresponding index  of cell type, as the image label )"""

df_original = pd.read_csv(os.path.join(data_dir, 'HAM10000_metadata.csv'))
df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)
df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)
df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes
df_original.head()

# this will tell us how many images are associated with each lesion_id
df_undup = df_original.groupby('lesion_id').count()
# now we filter out lesion_id's that have only one image associated with it
df_undup = df_undup[df_undup['image_id'] == 1]
df_undup.reset_index(inplace=True)
df_undup.head()

# here we identify lesion_id's that have duplicate images and those that have only one image.
def get_duplicates(x):
    unique_list = list(df_undup['lesion_id'])
    if x in unique_list:
        return 'unduplicated'
    else:
        return 'duplicated'

# create a new colum that is a copy of the lesion_id column
df_original['duplicates'] = df_original['lesion_id']
# apply the function to this new column
df_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)
df_original.head()

df_original['duplicates'].value_counts()

# now we filter out images that don't have duplicates
df_undup = df_original[df_original['duplicates'] == 'unduplicated']
df_undup.shape

# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set
y = df_undup['cell_type_idx']
_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)
df_val.shape

df_val['cell_type_idx'].value_counts()

# This set will be df_original excluding all rows that are in the val set
# This function identifies if an image is part of the train or val set.
def get_val_rows(x):
    # create a list of all the lesion_id's in the val set
    val_list = list(df_val['image_id'])
    if str(x) in val_list:
        return 'val'
    else:
        return 'train'

# identify train and val rows
# create a new colum that is a copy of the image_id column
df_original['train_or_val'] = df_original['image_id']
# apply the function to this new column
df_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)
# filter out train rows
df_train = df_original[df_original['train_or_val'] == 'train']
print(len(df_train))
print(len(df_val))

df_train['cell_type_idx'].value_counts()

df_val['cell_type'].value_counts()

"""**From From the above statistics of each category, we can see that there is a serious class imbalance in the training data. To solve this problem, I think we can start from two aspects, one is equalization sampling, and the other is a loss function that can be used to mitigate category imbalance during training, such as focal loss.**"""

# Copy fewer class to balance the number of 7 classes
data_aug_rate = [15,10,5,50,0,40,5]


# for i in range(7):
#     if data_aug_rate[i]:
#         df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)
# df_train['cell_type'].value_counts()



import pandas as pd

# Assuming df_train is your initial DataFrame and data_aug_rate is defined as before

for i in range(7):
    if data_aug_rate[i]:
        df_to_append = pd.DataFrame(df_train.loc[df_train['cell_type_idx'] == i, :])
        # Repeat the DataFrame (data_aug_rate[i]-1) times and concat
        df_augmented = pd.concat([df_to_append]*data_aug_rate[i], ignore_index=True)
        df_train = pd.concat([df_train, df_augmented], ignore_index=True)

# Checking the value counts after augmentation
print(df_train['cell_type'].value_counts())



"""At the beginning, I divided the data into three parts, training set, validation set and test set. Considering the small amount of data, I did not further divide the validation set data in practice."""

# # We can split the test set again in a validation set and a true test set:
# df_val, df_test = train_test_split(df_val, test_size=0.5)
df_train = df_train.reset_index()
df_val = df_val.reset_index()
# df_test = df_test.reset_index()

"""## Step 2. Model building"""

# feature_extract is a boolean that defines if we are finetuning or feature extracting.
# If feature_extract = False, the model is finetuned and all model parameters are updated.
# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.

# resnet,vgg,densenet,inception
# model_name = 'MobileViTV2ForImageClassification'  # resnet used_model
num_classes = 7
feature_extract = False
device = torch.device('cuda')
learning_rate = 1e-4
batch_size = 32
print("learning rate  :", learning_rate )

print("batch_size :", batch_size)


input_size = 224

# norm_mean = (0.49139968, 0.48215827, 0.44653124)
# norm_std = (0.24703233, 0.24348505, 0.26158768)
# define the transformation of the train images.
train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),
                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),
                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),
                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])
# define the transformation of the val images.
val_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),
                                    transforms.Normalize(norm_mean, norm_std)])

# Define a pytorch dataloader for this dataset
class HAM10000(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        # Load data and get label
        X = Image.open(self.df['path'][index])
        y = torch.tensor(int(self.df['cell_type_idx'][index]))

        if self.transform:
            X = self.transform(X)

        return X, y
    


# Define the training set using the table train_df and using our defined transitions (train_transform)
training_set = HAM10000(df_train, transform=train_transform)
train_loader = DataLoader(training_set, batch_size= batch_size, shuffle=True, num_workers=4)
# Same for the validation set:
validation_set = HAM10000(df_val, transform=train_transform)
val_loader = DataLoader(validation_set, batch_size= batch_size, shuffle=False, num_workers=4)

# this function is used during training process, to calculation the loss and accuracy
class AverageMeter(object):
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def train_test_plot(model_name = 'resnet'):
    print("\n\n\n\n ## running model      :   ", model_name,"       ## " )

    model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)
    model = model_ft.to(device)
    
    optimizer = optim.Adam(model.parameters(), lr= learning_rate)  # 1e-3
    criterion = nn.CrossEntropyLoss().to(device)
    
    total_loss_train, total_acc_train = [],[]
    
    def train(train_loader, model, criterion, optimizer, epoch):
        model.train()
        train_loss = AverageMeter()
        train_acc = AverageMeter()
        curr_iter = (epoch - 1) * len(train_loader)
        for i, data in enumerate(train_loader):
            images, labels = data
            N = images.size(0)
            # print('image shape:',images.size(0), 'label shape',labels.size(0))
            images = Variable(images).to(device)
            labels = Variable(labels).to(device)
    
            optimizer.zero_grad()
            outputs = model(images)
            
            # outputs = outputs1.logits
            
            # if isinstance(outputs, torch.Tensor):
            # # If `outputs` is already a torch.Tensor, no further action is needed
            #     loss = criterion(outputs, labels)

            # elif isinstance(outputs, ImageClassifierOutput):
            #     # If `outputs` is of type ImageClassifierOutput, extract the logits
            #     outputs = outputs1.logits
            #     loss = criterion(outputs, labels)
                
            if isinstance(outputs, torch.Tensor):
                # Direct tensor output
                loss = criterion(outputs, labels)
            elif hasattr(outputs, 'logits'):
                # Output has a logits attribute, typical of structured model outputs
                loss = criterion(outputs.logits, labels)
                outputs = outputs.logits
                
            loss.backward()
            optimizer.step()
            prediction = outputs.max(1, keepdim=True)[1]
            train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)
            train_loss.update(loss.item())
            curr_iter += 1
            if (i + 1) % 1000 == 0:
                print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (
                    epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))
                total_loss_train.append(train_loss.avg)
                total_acc_train.append(train_acc.avg)

        return train_loss.avg, train_acc.avg
    
    def validate(val_loader, model, criterion, optimizer, epoch):
        model.eval()
        val_loss = AverageMeter()
        val_acc = AverageMeter()
        with torch.no_grad():
            for i, data in enumerate(val_loader):
                images, labels = data
                N = images.size(0)
                images = Variable(images).to(device)
                labels = Variable(labels).to(device)
    
                outputs = model(images)
                # outputs = outputs1.logits
                if isinstance(outputs, torch.Tensor):
                    # Direct tensor output
                    loss = criterion(outputs, labels)
                elif hasattr(outputs, 'logits'):
                    # Output has a logits attribute, typical of structured model outputs
                    loss = criterion(outputs.logits, labels)
                    outputs = outputs.logits

    
                prediction = outputs.max(1, keepdim=True)[1]
    
                val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)
    
                val_loss.update(criterion(outputs, labels).item())
    
        # print('------------------------------------------------------------')
        print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))
        # print('------------------------------------------------------------')
        return val_loss.avg, val_acc.avg
    
    epochs = 30
    best_val_acc = 0
    total_loss_val, total_acc_val = [],[]
    epoch_list=[]

    for epoch in range(1, epochs+1):
        loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)
        loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)
        total_loss_val.append(loss_val)
        total_acc_val.append(acc_val)
        epoch_list.append(epoch)

        if acc_val > best_val_acc:
            best_val_acc = acc_val
            print('*****************************************************')
            print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))
            print('*****************************************************')
    
    """## Step 4. Model evaluation"""
    
    # fig = plt.figure(num = 2)
    # fig1 = fig.add_subplot(2,1,1)
    # fig2 = fig.add_subplot(2,1,2)
    # fig1.plot(total_loss_train, label = 'training loss')
    # fig1.plot(total_acc_train, label = 'training accuracy')
    # fig2.plot(total_loss_val, label = 'validation loss')
    # fig2.plot(total_acc_val, label = 'validation accuracy')
    # plt.legend()
    # plt.tight_layout()
    # plt.savefig(f"{model_name}_loss_accuracy_curve.png", dpi=300) 
    # plt.show()
    
    
    import matplotlib.pyplot as plt
    
    # Assuming total_acc_train and total_acc_val might be in decimal form. Convert to percentage if necessary.
    total_acc_train_percent = [acc * 100 for acc in total_acc_train]
    total_acc_val_percent = [acc * 100 for acc in total_acc_val]
    
    # Set global parameters to ensure consistency and readability in IEEE format
    plt.rcParams.update({'font.size': 10, 'figure.figsize': (3.5, 2.5)})
    
    # Plot for Loss
    plt.figure(figsize=(3.5 * 1.5, 2.8 * 1.5))
    plt.plot(epoch_list, total_loss_train, label="Train Loss")
    plt.plot(epoch_list, total_loss_val, label="Validation Loss")
    plt.legend(fontsize=10)
    plt.xlabel('Epoch Number', fontsize=10)
    plt.ylabel('Loss', fontsize=10)
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)
    plt.title(f' Loss for {model_name}', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{model_name}_loss_curve.png", dpi=300)
    plt.show()
    
    # Plot for Accuracy
    plt.figure(figsize=(3.5 * 1.5, 2.8 * 1.5))
    plt.plot(epoch_list, total_acc_train_percent, label="Train Accuracy")
    plt.plot(epoch_list, total_acc_val_percent, label="Validation Accuracy")
    plt.legend(fontsize=10)
    plt.xlabel('Epoch Number', fontsize=10)
    plt.ylabel('Accuracy', fontsize=10)
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)
    plt.title(f'Accuracy Curve for  {model_name}', fontsize=12)
    plt.ylim(0, 100)  # Set the y-axis range from 0 to 100
    plt.tight_layout()
    plt.savefig(f"{model_name}_accuracy_ieee.png", dpi=300)
    plt.show()
    
    
    def plot_confusion_matrix(cm, classes,
                              normalize=False,
                              title='Confusion matrix',
                              cmap=plt.cm.Blues):
        """
        This function prints and plots the confusion matrix.
        Normalization can be applied by setting `normalize=True`.
        """
        plt.imshow(cm, interpolation='nearest', cmap=cmap)
        plt.title(title)
        plt.colorbar()
        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes, rotation=45)
        plt.yticks(tick_marks, classes)
    
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
        thresh = cm.max() / 2.
        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            plt.text(j, i, cm[i, j],
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
    
        plt.tight_layout()
        plt.ylabel('True label')
        plt.xlabel('Predicted label')
    
    model.eval()
    y_label = []
    y_predict = []
    with torch.no_grad():
        for i, data in enumerate(val_loader):
            images, labels = data
            N = images.size(0)
            images = Variable(images).to(device)
            labels = Variable(labels).to(device)

            outputs = model(images)
            # outputs = outputs1.logits
            if isinstance(outputs, torch.Tensor):
                # Direct tensor output
                loss = criterion(outputs, labels)
            elif hasattr(outputs, 'logits'):
                # Output has a logits attribute, typical of structured model outputs
                loss = criterion(outputs.logits, labels)
                outputs = outputs.logits


            prediction = outputs.max(1, keepdim=True)[1]
            y_label.extend(labels.cpu().numpy())
            y_predict.extend(np.squeeze(prediction.cpu().numpy().T))
    
    # compute the confusion matrix
    confusion_mtx = confusion_matrix(y_label, y_predict)
    # plot the confusion matrix
    plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']
    plot_confusion_matrix(confusion_mtx, plot_labels)
    plt.savefig(f"{model_name}_confusion_matrix.png", dpi=300)
    plt.show()
    
    # Generate a classification report
    report = classification_report(y_label, y_predict, target_names=plot_labels)
    print(report)
    
    true_labels = y_label
    predictions = y_predict
    # Calculate metrics
    precision = precision_score(true_labels, predictions, average='macro')
    recall = recall_score(true_labels, predictions, average='macro')
    f1 = f1_score(true_labels, predictions, average='macro')
    accuracy = accuracy_score(true_labels, predictions)
    
    # Print metrics
    print(f'== On test data ==')
    
    print(f'Test Accuracy: {accuracy * 100:.2f}%')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1 Score: {f1:.4f}')
    
    label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)
    plt.bar(np.arange(7),label_frac_error)
    plt.xlabel('True Label')
    plt.ylabel('Fraction classified incorrectly')

    print("finished with :", model_name)


# train_test_plot(model_name = 'resnet')

train_test_plot(model_name = 'ViTForImageClassification2')
train_test_plot(model_name = 'ConvNextV2ForImageClassification')
train_test_plot(model_name = 'Swinv2ForImageClassification')
train_test_plot(model_name = 'CvtForImageClassification')
train_test_plot(model_name = 'EfficientFormerForImageClassification')
train_test_plot(model_name = 'PvtV2ForImageClassification')
train_test_plot(model_name = 'MobileViTV2ForImageClassification')

print("\nExecution Finished")
